{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jHNgZ3IUEt-5",
        "cfXuKVC7EiOS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Project Title: Building a Simple Information Retrieval System using BM25 and GPT-3 and evaluated in the CISI collection.**\n",
        "\n",
        ">**Project Overview:**\n",
        "\n",
        ">The aim of this project is to introduce students to the basics of Information Retrieval (IR) and the BM25 ranking algorithm. The sollution to this project will help us to select the students to attend the course on deep learning applied to search. The students will build a simple IR system that retrieves relevant documents from a pre-defined corpus based on the user's query. The students will also use the chatGPT to assist in all phases of the project. The coding will be done using Google Colab notebooks and the deliverables will be placed in github account.\n",
        "\n",
        ">**Project Outcomes:**\n",
        "\n",
        "> Upon completion of the project, students will have gained the following skills:\n",
        "\n",
        ">Understanding of the BM25 ranking algorithm\n",
        "Familiarity with Information Retrieval systems\n",
        ">Use of GitHub to store the notebooks and documentation\n",
        "Experience with chatGPT for help in find solutions and coding\n",
        "Exposure to Google Colab notebooks for code execution and presentation.\n"
      ],
      "metadata": {
        "id": "diqeI7fioNWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Files\n",
        "> Download dos arquivos da coleção de teste CISI"
      ],
      "metadata": {
        "id": "jHNgZ3IUEt-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e988M1SlEyKt",
        "outputId": "d0ee252d-1ebb-4cbe-d296-f199317211cc"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-02 01:21:45--  https://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
            "Resolving ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)... 130.209.240.253\n",
            "Connecting to ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)|130.209.240.253|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 775144 (757K) [application/gzip]\n",
            "Saving to: ‘cisi.tar.gz’\n",
            "\n",
            "cisi.tar.gz         100%[===================>] 756.98K  1.67MB/s    in 0.4s    \n",
            "\n",
            "2023-03-02 01:21:46 (1.67 MB/s) - ‘cisi.tar.gz’ saved [775144/775144]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf cisi.tar.gz"
      ],
      "metadata": {
        "id": "CWz8LDaPGO50"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & Utils\n",
        "> Definições de funções usadas para manipulação dos dados da coleção CISI"
      ],
      "metadata": {
        "id": "YIf6HuKxCGnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "7VRoJE3ECGHZ"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path:str=\"\"):\n",
        "  \"\"\"\n",
        "  Reading CISI collection files and converting them to a list of strings\n",
        "  \"\"\"\n",
        "  with open(path) as f:\n",
        "        lines = \"\"\n",
        "        for l in f.readlines():\n",
        "            lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
        "        lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
        "  return lines\n",
        "\n",
        "def extract_docs(lines:list=None):\n",
        "    \"\"\"\n",
        "    Processes the CISI.ALL file and returns a dictionary with the fields: \n",
        "    id (it is a numerical integer referring to the id of the document), \n",
        "    and document written in natural language.\n",
        "    \"\"\"\n",
        "    doc_set = {}\n",
        "    doc_id = \"\"\n",
        "    doc_text = \"\"\n",
        "\n",
        "    for l in lines:\n",
        "        if l.startswith(\".I\"):\n",
        "            doc_id = l.split(\" \")[1].strip() \n",
        "        elif l.startswith(\".X\"):\n",
        "            doc_set[doc_id] = doc_text.lstrip(\" \")\n",
        "            doc_id = \"\"\n",
        "            doc_text = \"\"\n",
        "        else:\n",
        "            doc_text += re.sub(r\"[^a-zA-Z]+\", ' ', l.strip()[3:] + \" \").strip() \n",
        "\n",
        "\n",
        "    print(f\"Number of documents = {len(doc_set)}\")\n",
        "    print(\"Doc id: \", 1) \n",
        "    print(\"Doc content: \", doc_set[\"1\"]) \n",
        "    return doc_set\n",
        "\n",
        "def extract_queries(lines:list=None):\n",
        "    \"\"\"    \n",
        "    Processes the CISI.QRY file and returns a dictionary with the fields: \n",
        "    id (it is a numerical integer referring to the id of the Query), \n",
        "    and query written in natural language.\n",
        "    \"\"\"\n",
        "    qry_set = {}\n",
        "    qry_id = \"\"\n",
        "    for l in lines:\n",
        "        if l.startswith(\".I\"):\n",
        "            qry_id = l.split(\" \")[1].strip() \n",
        "        elif l.startswith(\".W\"):\n",
        "            qry_set[qry_id] = l.strip()[3:]\n",
        "            qry_id = \"\"\n",
        "\n",
        "    print(f\"\\n\\nNumber of queries = {len(qry_set)}\")\n",
        "    print(\"Query id: \", 1) \n",
        "    print(\"Query content: \", qry_set[\"1\"])     \n",
        "    return qry_set\n",
        "\n",
        "def extract_rel(path:str=\"\"):\n",
        "    \"\"\"\n",
        "    Reading and processing the CISI.REL file. \n",
        "    The function returns a dictionary with a key being the query id and a \n",
        "    list of relevant document ids as a value.\n",
        "    \"\"\"\n",
        "    rel_set = {}\n",
        "    with open(path) as f:\n",
        "        for l in f.readlines():\n",
        "            qry_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0] \n",
        "            doc_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1]\n",
        "\n",
        "            if qry_id in rel_set:\n",
        "                rel_set[qry_id].append(doc_id)\n",
        "            else:\n",
        "                rel_set[qry_id] = []\n",
        "                rel_set[qry_id].append(doc_id)\n",
        "\n",
        "    print(f\"\\n\\nNumber of mappings = {len(rel_set)}\")\n",
        "    print(\"Ids of the relevant documents of the query 1\", rel_set[\"1\"])  \n",
        "    return rel_set\n",
        "\n",
        "def extract_files(path:str=\"\"):\n",
        "  \"\"\"\n",
        "  From a directory, extract the contents of the files:\n",
        "  CISI.ALL, CISI.QRY and CISI.REL\n",
        "  \"\"\"\n",
        "  # extract all documents\n",
        "  doc_set = extract_docs(load_data(os.path.join(path, 'CISI.ALL')))\n",
        "\n",
        "  #Extract all Queries\n",
        "  qry_set = extract_queries(load_data(os.path.join(path, 'CISI.QRY')))\n",
        "\n",
        "  # Extract all related doc and Queries\n",
        "  rel_set = extract_rel(os.path.join(path, 'CISI.REL'))\n",
        "  \n",
        "  return doc_set, qry_set, rel_set\n",
        "\n",
        "def save_docs_to_json(docs):\n",
        "  \"\"\"\n",
        "  Converts the document set into a json mapping. \n",
        "  This mapping will be used by the pyserini implementation\n",
        "  \"\"\"\n",
        "  lucene_dataset = pd.DataFrame(docs.items(),columns=['id','contents'], index=None)\n",
        "  lucene_json = lucene_dataset.to_json(orient=\"records\")\n",
        "  luceneJsonObject = json.loads(lucene_json)\n",
        "  json_object = json.dumps(luceneJsonObject, indent = 4) \n",
        "  with open('lucene.json', 'w') as outfile: \n",
        "      outfile.write(json_object)"
      ],
      "metadata": {
        "id": "e8Yaf8aJBvFR"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract the contents\n",
        "doc_set, qry, rel = extract_files('/content/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN339yzFHLTo",
        "outputId": "00c6926b-979f-4826-9431-2c3124af9680"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents = 1460\n",
            "Doc id:  1\n",
            "Doc content:  18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n",
            "\n",
            "\n",
            "Number of queries = 112\n",
            "Query id:  1\n",
            "Query content:  What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
            "\n",
            "\n",
            "Number of mappings = 76\n",
            "Ids of the relevant documents of the query 1 ['28', '35', '38', '42', '43', '52', '65', '76', '86', '150', '189', '192', '193', '195', '215', '269', '291', '320', '429', '465', '466', '482', '483', '510', '524', '541', '576', '582', '589', '603', '650', '680', '711', '722', '726', '783', '813', '820', '868', '869', '894', '1162', '1164', '1195', '1196', '1281']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_docs_to_json(doc_set)"
      ],
      "metadata": {
        "id": "IynYiFmCJ5Oc"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Organizing the directories\n",
        "!mkdir '/content/samples_to_bm25'\n",
        "!mkdir '/content/output_bm25'\n",
        "!mv '/content/lucene.json' '/content/samples_to_bm25/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJIxyjUiMO0_",
        "outputId": "9abf7653-3299-4abf-8e40-25bff8e9c3c6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/samples_to_bm25’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing with BM25\n",
        "O BM25 é um dos algoritmos mais usados para a construção de sistemas de IR. Ele é categorizado como uma recuperação esparsa e usa representações bag-of-words para a indexação dos documentos.\n",
        "\n",
        "A implementação usada aqui é do Pyserini\n",
        ">https://github.com/castorini/pyserini"
      ],
      "metadata": {
        "id": "44CrpIWip3pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages\n",
        "> Instação Pyserini + rank-eval (usado para avaliação de sistemas IR) "
      ],
      "metadata": {
        "id": "cfXuKVC7EiOS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1ECHCugwMTAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd0e7947-df02-4ea2-8064-90b28a8be0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libomp-10-dev libomp5-10\n",
            "Suggested packages:\n",
            "  libomp-10-doc\n",
            "The following NEW packages will be installed:\n",
            "  libomp-10-dev libomp-dev libomp5-10\n",
            "0 upgraded, 3 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 351 kB of archives.\n",
            "After this operation, 2,281 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libomp5-10 amd64 1:10.0.0-4ubuntu1 [300 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libomp-10-dev amd64 1:10.0.0-4ubuntu1 [47.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libomp-dev amd64 1:10.0-50~exp1 [2,824 B]\n",
            "Fetched 351 kB in 1s (554 kB/s)\n",
            "Selecting previously unselected package libomp5-10:amd64.\n",
            "(Reading database ... 128208 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5-10_1%3a10.0.0-4ubuntu1_amd64.deb ...\n",
            "Unpacking libomp5-10:amd64 (1:10.0.0-4ubuntu1) ...\n",
            "Selecting previously unselected package libomp-10-dev.\n",
            "Preparing to unpack .../libomp-10-dev_1%3a10.0.0-4ubuntu1_amd64.deb ...\n",
            "Unpacking libomp-10-dev (1:10.0.0-4ubuntu1) ...\n",
            "Selecting previously unselected package libomp-dev.\n",
            "Preparing to unpack .../libomp-dev_1%3a10.0-50~exp1_amd64.deb ...\n",
            "Unpacking libomp-dev (1:10.0-50~exp1) ...\n",
            "Setting up libomp5-10:amd64 (1:10.0.0-4ubuntu1) ...\n",
            "Setting up libomp-10-dev (1:10.0.0-4ubuntu1) ...\n",
            "Setting up libomp-dev (1:10.0-50~exp1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "/content\n",
            "\u001b[33mWARNING: Skipping pygaggle as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCloning into 'pygaggle'...\n",
            "remote: Enumerating objects: 1557, done.\u001b[K\n",
            "remote: Counting objects: 100% (697/697), done.\u001b[K\n",
            "remote: Compressing objects: 100% (221/221), done.\u001b[K\n",
            "remote: Total 1557 (delta 592), reused 503 (delta 476), pack-reused 860\u001b[K\n",
            "Receiving objects: 100% (1557/1557), 509.49 KiB | 18.87 MiB/s, done.\n",
            "Resolving deltas: 100% (998/998), done.\n",
            "Submodule 'tools' (https://github.com/castorini/anserini-tools.git) registered for path 'tools'\n",
            "Cloning into '/content/pygaggle/tools'...\n",
            "remote: Enumerating objects: 723, done.        \n",
            "remote: Counting objects: 100% (480/480), done.        \n",
            "remote: Compressing objects: 100% (418/418), done.        \n",
            "remote: Total 723 (delta 74), reused 453 (delta 61), pack-reused 243        \n",
            "Receiving objects: 100% (723/723), 57.78 MiB | 24.72 MiB/s, done.\n",
            "Resolving deltas: 100% (158/158), done.\n",
            "Submodule path 'tools': checked out '808f48711b5e172da6aec8b1855518c8ea65489f'\n",
            "/content/pygaggle\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.6/489.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 KB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "xarray-einstats 0.5.1 requires scipy>=1.6, but you have scipy 1.5.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m252.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pyserini faiss intel-openmp nltk --quiet\n",
        "!apt install libomp-dev\n",
        "%cd /content\n",
        "!rm -rf pygaggle && pip uninstall -y pygaggle\n",
        "!git clone  --recursive https://github.com/castorini/pygaggle.git\n",
        "%cd pygaggle\n",
        "! pip install --editable . --quiet\n",
        "! pip install gensim==4.2.0 jsonlines --quiet\n",
        "! pip install faiss-cpu --no-cache --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank-eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9hdUXzYXzHK",
        "outputId": "05505ae4-7e25-4bb2-ce95-01d8c56db5c0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rank-eval\n",
            "  Downloading rank_eval-0.1.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from rank-eval) (0.8.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rank-eval) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from rank-eval) (4.56.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from rank-eval) (1.5.3)\n",
            "Requirement already satisfied: numba>=0.54.1 in /usr/local/lib/python3.8/dist-packages (from rank-eval) (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54.1->rank-eval) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.54.1->rank-eval) (6.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.54.1->rank-eval) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->rank-eval) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->rank-eval) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->rank-eval) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.54.1->rank-eval) (3.15.0)\n",
            "Installing collected packages: rank-eval\n",
            "Successfully installed rank-eval-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying BM25 via lucene implementation"
      ],
      "metadata": {
        "id": "ASw00iPsTxf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m pyserini.index.lucene \\\n",
        "  --collection JsonCollection \\\n",
        "  --input /content/samples_to_bm25 \\\n",
        "  --index /content/output_bm25 \\\n",
        "  --generator DefaultLuceneDocumentGenerator \\\n",
        "  --threads 2 \\\n",
        "  --storePositions --storeDocvectors --storeRaw"
      ],
      "metadata": {
        "id": "_0Fu_bD8qDNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1756e3-95ef-4ccc-d159-012b40350e7f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
            "2023-03-01 12:44:16,575 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Setting log level to INFO\n",
            "2023-03-01 12:44:16,578 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Starting indexer...\n",
            "2023-03-01 12:44:16,579 INFO  [main] index.IndexCollection (IndexCollection.java:395) - ============ Loading Parameters ============\n",
            "2023-03-01 12:44:16,579 INFO  [main] index.IndexCollection (IndexCollection.java:396) - DocumentCollection path: /content/samples_to_bm25\n",
            "2023-03-01 12:44:16,580 INFO  [main] index.IndexCollection (IndexCollection.java:397) - CollectionClass: JsonCollection\n",
            "2023-03-01 12:44:16,584 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Generator: DefaultLuceneDocumentGenerator\n",
            "2023-03-01 12:44:16,585 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Threads: 2\n",
            "2023-03-01 12:44:16,586 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Language: en\n",
            "2023-03-01 12:44:16,586 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Stemmer: porter\n",
            "2023-03-01 12:44:16,587 INFO  [main] index.IndexCollection (IndexCollection.java:402) - Keep stopwords? false\n",
            "2023-03-01 12:44:16,587 INFO  [main] index.IndexCollection (IndexCollection.java:403) - Stopwords: null\n",
            "2023-03-01 12:44:16,588 INFO  [main] index.IndexCollection (IndexCollection.java:404) - Store positions? true\n",
            "2023-03-01 12:44:16,592 INFO  [main] index.IndexCollection (IndexCollection.java:405) - Store docvectors? true\n",
            "2023-03-01 12:44:16,593 INFO  [main] index.IndexCollection (IndexCollection.java:406) - Store document \"contents\" field? false\n",
            "2023-03-01 12:44:16,594 INFO  [main] index.IndexCollection (IndexCollection.java:407) - Store document \"raw\" field? true\n",
            "2023-03-01 12:44:16,595 INFO  [main] index.IndexCollection (IndexCollection.java:408) - Additional fields to index: []\n",
            "2023-03-01 12:44:16,596 INFO  [main] index.IndexCollection (IndexCollection.java:409) - Optimize (merge segments)? false\n",
            "2023-03-01 12:44:16,597 INFO  [main] index.IndexCollection (IndexCollection.java:410) - Whitelist: null\n",
            "2023-03-01 12:44:16,598 INFO  [main] index.IndexCollection (IndexCollection.java:411) - Pretokenized?: false\n",
            "2023-03-01 12:44:16,602 INFO  [main] index.IndexCollection (IndexCollection.java:412) - Index path: /content/output_bm25\n",
            "2023-03-01 12:44:16,608 INFO  [main] index.IndexCollection (IndexCollection.java:450) - ============ Indexing Collection ============\n",
            "2023-03-01 12:44:17,441 INFO  [main] index.IndexCollection (IndexCollection.java:565) - Thread pool with 2 threads initialized.\n",
            "2023-03-01 12:44:17,442 INFO  [main] index.IndexCollection (IndexCollection.java:567) - Initializing collection in /content/samples_to_bm25\n",
            "2023-03-01 12:44:17,446 INFO  [main] index.IndexCollection (IndexCollection.java:576) - 1 file found\n",
            "2023-03-01 12:44:17,446 INFO  [main] index.IndexCollection (IndexCollection.java:577) - Starting to index...\n",
            "2023-03-01 12:44:19,018 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - samples_to_bm25/lucene.json: 1460 docs added.\n",
            "2023-03-01 12:44:19,497 INFO  [main] index.IndexCollection (IndexCollection.java:633) - Indexing Complete! 1,460 documents indexed\n",
            "2023-03-01 12:44:19,497 INFO  [main] index.IndexCollection (IndexCollection.java:634) - ============ Final Counter Values ============\n",
            "2023-03-01 12:44:19,497 INFO  [main] index.IndexCollection (IndexCollection.java:635) - indexed:            1,460\n",
            "2023-03-01 12:44:19,498 INFO  [main] index.IndexCollection (IndexCollection.java:636) - unindexable:            0\n",
            "2023-03-01 12:44:19,498 INFO  [main] index.IndexCollection (IndexCollection.java:637) - empty:                  0\n",
            "2023-03-01 12:44:19,498 INFO  [main] index.IndexCollection (IndexCollection.java:638) - skipped:                0\n",
            "2023-03-01 12:44:19,498 INFO  [main] index.IndexCollection (IndexCollection.java:639) - errors:                 0\n",
            "2023-03-01 12:44:19,505 INFO  [main] index.IndexCollection (IndexCollection.java:642) - Total 1,460 documents indexed in 00:00:02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate BM25\n",
        ">Para isso iremos recuperar documentos para cada query e verificar a posição que o melhor documento é retornado.\n",
        "> Metricas usadas: **MRR** e **nDCG**\n",
        "\n",
        "A métrica **MRR** (Mean Reciprocal Rank) mede o quão bem um sistema de IR classifica as respostas mais relevantes em primeiro lugar.\n",
        "\n",
        "O MRR é calculado dividindo 1 pelo rank da primeira resposta relevante.\n",
        "\n",
        "O objetivo é obter um MRR mais próximo de 1, o que indica que o sistema de IR está colocando as respostas mais relevantes no topo da lista com mais frequência\n",
        "\n",
        "Já a Métrica **nDGC** (normalized Discounted Cumulative Gain) mede o grau em que as respostas retornadas para uma consulta estão classificadas corretamente de acordo com sua relevância. Para fazer isso, ela considera a relevância das respostas e sua posição na lista de resultados.\n",
        "\n"
      ],
      "metadata": {
        "id": "MhvUR-U7NJw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.search.lucene import LuceneSearcher\n",
        "from pygaggle.rerank.base import hits_to_texts\n",
        "\n",
        "def get_results(path_out_bm25='/content/output_bm25', qrys=None, k=100):\n",
        "  searcher = LuceneSearcher(path_out_bm25)\n",
        "  results = {}\n",
        "  for key, value in qrys.items():\n",
        "    results_found = [found.docid for found in searcher.search(value, k)]\n",
        "    scores = [found.score for found in searcher.search(value, k)]\n",
        "    if key in rel:\n",
        "      results[key]  = {'founds':results_found, 'true':rel[key], 'scores':scores}\n",
        "  return results\n",
        "\n"
      ],
      "metadata": {
        "id": "HvOZnrVINOJF"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = get_results(qrys=qry)\n",
        "print(\"Total results: \", len(results))\n",
        "print(\"Total docs ids: \", len(results['2']['founds']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-TEhpAPIQkX",
        "outputId": "7a573782-488d-415f-a738-02955c2383c9"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total results:  76\n",
            "Total docs ids:  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_eval import Qrels, Run, evaluate\n",
        "\n",
        "#Configs vars\n",
        "qrels = Qrels()\n",
        "qrels.add_multi(q_ids=rel.keys(),\n",
        "                doc_ids=[v for k, v in rel.items()],\n",
        "                scores=[[1]*len(v) for k, v in rel.items()])\n",
        "run = Run()\n",
        "run.add_multi(\n",
        "    q_ids=results.keys(),\n",
        "    doc_ids=[results[k][\"founds\"] for k in results.keys()],\n",
        "    scores=[results[k][\"scores\"] for k in results.keys()],\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "F14s4DdgX3pP"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute scores for multiple metrics at once\n",
        "evaluate(qrels, run, [\"mrr\",\"ndcg@10\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVIARFYQdZN6",
        "outputId": "559272e5-ef95-4c49-b419-ec5328642066"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mrr': 0.612313359700076, 'ndcg@10': 0.360460986561388}"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise\n",
        "\n",
        "> Nesta análise complementar, a ideia é verificar as queries que piores tiveram seus documentos ranqueados pelo BM25 e como trabalhos futuros tentar melhorar com o uso do ChatGPT3"
      ],
      "metadata": {
        "id": "_EYzuAL37Kzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_ranks_qid = {}\n",
        "for k in results.keys():\n",
        "  ranks = []\n",
        "  for d_rel in results[k]['true']:\n",
        "    if d_rel in results[k]['founds']:\n",
        "      ranks.append(results[k]['founds'].index(d_rel))\n",
        "  ranks.sort()\n",
        "  best_ranks_qid[k] = ranks[0]\n",
        "\n",
        "#Queries that got the worst rank by BM25\n",
        "sorted(best_ranks_qid.items(), key=lambda x:x[1], reverse=True)[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrDlts2B7J7p",
        "outputId": "e556bfbb-532a-4f28-ff68-c72ddca60e2a"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('14', 96), ('101', 75), ('61', 43), ('8', 40), ('43', 30)]"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que a Querie de id **14** e **101** foram as piores ranqueadas."
      ],
      "metadata": {
        "id": "wM6j8_hZLdMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results['14']['true']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1z8DInbP_Ke",
        "outputId": "925174bc-c2c8-49e7-bdeb-65673fb37ea0"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['45', '420', '890']"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_set['45']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "X0-YZnH6QCsz",
        "outputId": "08ae96e1-bd20-4145-da80-ef12b2493ba4"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Measuring the Reliability of Subject Classification by Men and Machines Borko, Harold Procedures for measuring the consistency of document classification are described.. Three subject specialists classified 997 abstracts of psychological reports into one of eleven categories.. These abstracts were also mechanically classified by a computer program using a factor score computational procedure.. Each abstract was scored for all categories and assigned to the one with the highest score.. The three manual classifications were compared with each other and with the mechanical classifications, and a series of contingency coefficients was computed.. The average reliability of manual classification procedures was equal to .870.. The correlation between automatic and manual classification was .766.. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_set['420']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "HMKZLeinQLC1",
        "outputId": "85663ce9-f835-48a2-ac56-065a1b8a1db5"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Automatic Creation of Literature Abstracts Luhn, H.P. Experts of technical papers and magazine articles that serve the purposes of conventional abstracts have been created entirely by automatic means.. In the exploratory research described, the complete text of an article in machine- readable form is scanned by an IBM 704 data-processing machine and analyzed in accordance with a standard program..  Statistical information derived from word frequency and distribution is used by the machine to compute a relative measure of significance, first for individual words and then for sentences.. Sentences scoring highest in significance are extracted and printed out to become the \"auto-abstract\".. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_set['890']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "pZqo7GwMQTHq",
        "outputId": "a3b52127-b7eb-4f7d-cd2e-311d65b0323b"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pattern Recognition and Structure-Activity Relationship Studies.. Computer-Assisted Prediction of Antitumor Activity in Structurally Diverse Drugs in an Experimental Mouse Brain Tumor System Chu, K. C. Feldman, R. J. Shapiro, M. B. Hazard, G. F. Jr. Geran, R. I. This paper reports the application of pattern recognition and substructural analysis to the problem of predicting the antineoplastic activity of 24 test compounds in an experimental mouse brain tumor system based on 138 structurally diverse compounds tested in this tumor system.. The molecules were represented by three types of substructural fragments, the augmented atom, the heteropath, and the ring fragments.. Of the two pattern recognition methods used to predict the activity of the test compounds the nearest neighbor method predicted 83% correctly while the learning machine  method predicted 92% correctly.. The test structures and the important substructural fragments used in this study are given and the implications of these results are discussed.. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query 14:\", qry['14'])\n",
        "print(\"Top 1 and 2 by BM25\")\n",
        "print()\n",
        "print(doc_set[results['14']['founds'][0]])\n",
        "print('-----------------------')\n",
        "print(doc_set[results['14']['founds'][1]])\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAYj2jqr-U9G",
        "outputId": "3bc875d1-aa86-4584-f220-ba1410086029"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query 14: What future is there for automatic medical diagnosis?\n",
            "Top 1 and 2 by BM25\n",
            "\n",
            "Libraries and Technological Forces Affecting Them Cuadra, C.A. 1.  What do we mean by technology and what kind of technological forces are we concerned with? 2.  Why is it important to be concerned with technology in thinking about the future of libraries? 3.  What kinds of technology are particularly important for libraries? 4.  How can this technology be applied today? 5.  What can we foresee for the future, as we move toward the year 2000? 6.  What, if anything, should we do tomorrow to try to get from here to here? \n",
            "-----------------------\n",
            "Computer Indexing of Medical Articles - Project Medico Artandi, Susan An automatic indexing method is described in which index tags for documents are generated by the computer.. The computer scans the text of periodical articles and automatically assigns to them index terms with their respective weights on the basis of explicitly defined text characteristics.. A machine file of document references with their associated index terms is automatically produced which can be searched on a co-ordinate basis for the retrieval of specified drug-related information.. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report + uso ChatGPT3"
      ],
      "metadata": {
        "id": "M9dkEBjoYw4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste trabalho foi desenvolvido um sistema simples de ranqueamento de documentos usado o algoritmo BM25, implementado via Pyserini.\n",
        "\n",
        "A primeira etapa foi baixar a coleção de teste do CISI, extrai os documentos, querys e Qrels. A partir das informações mapeadas, foi indexado os documentos com o BM25 (LuceneSearcher) e logo depois foi avaliado a recuperação dos documentos.\n",
        "\n",
        "Para a avaliação foi interado cada query sobre a indexação feita na etapa anterior, para essa etapa também foi usado o pyserini, mais especificamente a função \"LuceneSearcher\"e atribuido um k, esse valor de k consiste na quantidade de documentos retornados de resposta por query, por padrão foi definido 100.\n",
        "\n",
        "Com a inferência feita em todas as querys foi  então computada a avaliação com as métricas MRR e nDCG. Como resultado vemos que em MRR foi obtido 61% enquando de nDCG igual a  0.36, os resultados apontam que é necessário melhoria no ranqueamento, a métrica MRR diz que a probabilidade do documento relevante estar próximo da primeira posição é de 61%.\n",
        "\n",
        "**Uso com o ChatGPT**\n",
        "\n",
        "Até o momento desse projeto o ChatGPT foi usado para entender melhor como o algoritmo do BM25 funciona; gerar explicações sobre as métricas MRR e nDCG e por fim foi tentado resumir alguns documentos, porém, não foi satisfatoria a saida via chatGPT.\n"
      ],
      "metadata": {
        "id": "VcAaiE74aCwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Output ChatGPT3**\n",
        "\n",
        "**Explain didactically how the BM25 works**\n",
        "\n",
        "The BM25 algorithm is a ranking function that is commonly used in information retrieval systems to retrieve relevant documents in response to a user's query. Here's how it works:\n",
        "\n",
        "Preprocessing: The first step is to preprocess the query and the document collection. This typically involves tokenization (breaking the text into individual words or terms), stopword removal (eliminating common words such as \"the\" and \"and\"), and stemming (reducing words to their base form, such as \"running\" to \"run\").\n",
        "\n",
        "Term frequency: For each term in the query and document collection, the algorithm calculates the number of times the term appears in each document. This is known as the term frequency (TF).\n",
        "\n",
        "Inverse document frequency: The algorithm also calculates the inverse document frequency (IDF) for each term. The IDF is a measure of how important a term is in the document collection. Terms that appear frequently in many documents (such as \"the\" or \"and\") have a low IDF, while terms that appear rarely in the document collection have a high IDF.\n",
        "\n",
        "BM25 score: The BM25 score is calculated for each document in the collection based on its similarity to the query. The score is based on the TF and IDF values for each term in the query and the document. The formula for the BM25 score is:\n",
        "\n",
        "score(D,Q) = sum(w(qi,D) * IDF(qi) * ((k1 + 1) * TF(qi,D)) / (k1 * (1 - b + b * (|D| / avgdl)) + TF(qi,D)))\n",
        "\n",
        "score(D,Q) is the BM25 score for document D given query Q\n",
        "w(qi,D) is a weighting factor that determines the importance of each query term\n",
        "IDF(qi) is the IDF for query term qi\n",
        "TF(qi,D) is the term frequency for term qi in document D\n",
        "k1 and b are tuning parameters that control the impact of term frequency and document length on the score\n",
        "|D| is the length of document D in terms\n",
        "avgdl is the average document length in terms\n",
        "The BM25 algorithm ranks documents based on their BM25 scores, with higher scores indicating a greater similarity to the query.\n",
        "\n",
        "In summary, the BM25 algorithm calculates a score for each document in the collection based on the similarity of its terms to those in the query, taking into account the importance of each term and the length of the document. This allows it to effectively retrieve relevant documents in response to a user's query."
      ],
      "metadata": {
        "id": "6LyR2uP_dBgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain in a didactic way the MRR and nDCG metrics for information retrieval**\n",
        "\n",
        "MRR (Mean Reciprocal Rank) and nDCG (Normalized Discounted Cumulative Gain) are evaluation metrics commonly used in information retrieval to measure the effectiveness of a system in returning relevant results to a user's query.\n",
        "\n",
        "MRR is a measure of how quickly the relevant results are returned by the system. It is calculated as the average of the reciprocal ranks of the first relevant document for each query. In other words, for each query, the reciprocal rank of the first relevant document is calculated (i.e., 1/rank) and then the average of these reciprocal ranks is taken across all queries. The higher the MRR, the more quickly the relevant results are returned by the system.\n",
        "\n",
        "nDCG is a measure of the quality of the results returned by the system. It takes into account both the relevance of the returned results and their rank in the list of results. It is calculated by first assigning a relevance score (usually a binary value) to each document based on how relevant it is to the query, and then calculating a cumulative gain score for the top k documents in the result list. The cumulative gain score is a measure of how much relevant information is contained in the top k documents, with greater weight given to documents that are higher in the result list. The cumulative gain score is then normalized by a discounted gain score, which gives greater weight to documents that are more relevant and appear higher in the result list. The nDCG score is the average of the normalized discounted cumulative gain scores across all queries. The higher the nDCG, the better the quality of the results returned by the system.\n",
        "\n",
        "In summary, MRR measures how quickly the relevant results are returned by the system, while nDCG measures the quality of the results returned by the system. Both metrics are important for evaluating the effectiveness of an information retrieval system in returning relevant results to a user's query.\n"
      ],
      "metadata": {
        "id": "fjQavc4TeM3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate a summary** \n",
        "\n",
        "**Measuring the Reliability of Subject Classification by Men and Machines Borko, Harold Procedures for measuring the consistency of document classification are described.. Three subject specialists classified 997 abstracts of psychological reports into one of eleven categories.. These abstracts were also mechanically classified by a computer program using a factor score computational procedure.. Each abstract was scored for all categories and assigned to the one with the highest score.. The three manual classifications were compared with each other and with the mechanical classifications, and a series of contingency coefficients was computed.. The average reliability of manual classification procedures was equal to .870.. The correlation between automatic and manual classification was .766..**\n",
        "\n",
        "> ChatGPT:\n",
        "\n",
        "> In this study, the reliability of document classification was measured using both manual and automated methods. Three subject specialists manually classified 997 abstracts of psychological reports into one of eleven categories, while a computer program used a factor score computational procedure to mechanically classify the same abstracts. Each abstract was scored for all categories and assigned to the one with the highest score. The manual classifications were compared with each other and with the mechanical classifications, and contingency coefficients were computed. The results showed that the average reliability of manual classification was .870, while the correlation between automatic and manual classification was .766."
      ],
      "metadata": {
        "id": "es6zTAOqeghl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8YqmQlVZdpXS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}